from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
import asyncio
from langchain_community.chat_models import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_chroma import Chroma
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
from pydantic import BaseModel
from datetime import datetime
import logging
from fastapi.middleware.cors import CORSMiddleware

# FastAPI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
app = FastAPI()


app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ë„ë©”ì¸ í—ˆìš© (í•„ìš” ì‹œ íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)



# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# LangChain ì²´ì¸ (ì „ì—­ ë³€ìˆ˜)
chain = None

# ìš”ì²­ ë°ì´í„° ëª¨ë¸
class ChatRequest(BaseModel):
    message: str
    croomIdx: int
    chatter: str
    ratings: str = "A"
    createdAt: datetime = datetime.now()

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ LangChain ì´ˆê¸°í™”"""
    global chain
    logger.info("ğŸš€ FastAPI ì„œë²„ ì‹œì‘ - LangChain ì´ˆê¸°í™” ì¤‘...")
    try:
        chain = create_chain()
        logger.info("âœ… LangChain ì´ˆê¸°í™” ì™„ë£Œ!")
    except Exception as e:
        logger.error(f"âŒ LangChain ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        chain = None

@app.on_event("shutdown")
async def shutdown_event():
    """ì„œë²„ ì¢…ë£Œ ì‹œ ë¡œê·¸ ì¶œë ¥"""
    logger.info("ğŸ›‘ FastAPI ì„œë²„ ì¢…ë£Œ...")

@app.post("/chat")
async def chat(request: ChatRequest):
    """LangChainì„ ì‚¬ìš©í•œ ë¬¸ì„œ ê¸°ë°˜ ê²€ìƒ‰ + ë‹µë³€ ìƒì„±"""
    global chain
    if chain is None:
        raise HTTPException(status_code=500, detail="LangChainì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    try:
        logger.info(f"ğŸ“¥ ìš”ì²­ ìˆ˜ì‹ : {request.message}")

        # ë¹„ë™ê¸° ì‹¤í–‰
        response = await asyncio.to_thread(chain.invoke, request.message)

        # Check if 'metadata' exists in the response
        response_text = response.content  # 'AIMessage' ê°ì²´ì—ì„œ content ì†ì„± ì¶”ì¶œ
        

        # ì´ì œ í…ìŠ¤íŠ¸ì—ì„œ ì¤„ ë°”ê¿ˆì„ ì²˜ë¦¬
        formatted_response = response_text.replace("\n", "<br>")

        return {
            "response": formatted_response,
            "croomIdx": request.croomIdx,
            "chatter": request.chatter,
            "ratings": request.ratings,
            "createdAt": request.createdAt
        }

    except Exception as e:
        logger.error(f"âŒ ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=str(e))





@app.get("/check_chroma")
async def check_chroma():
    """ChromaDB ìƒíƒœ í™•ì¸ API"""
    try:
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        embeddings_model = HuggingFaceEmbeddings(
            model_name='jhgan/ko-sroberta-nli',
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True},
        )
        logger.info("ğŸ” ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

        # ChromaDB ë¡œë“œ
        vectorstore = Chroma(
            persist_directory="./chroma_db",
            embedding_function=embeddings_model,
            collection_metadata={"hnsw:space": "cosine"},
        )

        logger.info("âœ… ChromaDB ë¡œë“œ ì„±ê³µ")

        # ë¬¸ì„œ ê°œìˆ˜ í™•ì¸
        documents = vectorstore.get(include=['metadatas'])
        doc_count = len(documents["metadatas"])
        logger.info(f"ğŸ“‚ ChromaDB ë‚´ ì €ì¥ëœ ë¬¸ì„œ ê°œìˆ˜: {doc_count}")

        return {"status": "success", "documents": doc_count}
    except Exception as e:
        logger.error(f"âŒ ChromaDB í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=f"ChromaDB í™•ì¸ ì˜¤ë¥˜: {e}")

def custom_retriever(query):
    # retrieverì—ì„œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ê¸°
    results = retriever._retrieve(query)  # _retrieve() ë©”ì„œë“œ ì‚¬ìš©

    documents_with_titles = []
    for result in results:
        # ë¬¸ì„œ ì œëª©ì„ ë©”íƒ€ë°ì´í„°ì—ì„œ ê°€ì ¸ì˜¤ê¸°
        doc_title = result.metadata.get("title", "No Title Available")  # 'title'ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ì„ 'No Title Available'ë¡œ ì„¤ì •
        documents_with_titles.append({
            "title": doc_title,
            "content": result.content
        })

    return documents_with_titles

def create_chain():
    """Create and return the LangChain chain with RAG, including document title."""
    try:
        # Initialize embeddings model
        embeddings_model = HuggingFaceEmbeddings(
            model_name='jhgan/ko-sroberta-nli',
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True},
        )
        logger.info("âœ… Embeddings model initialized.")

        # Initialize ChromaDB
        logger.info("ğŸ” Loading ChromaDB...")
        vectorstore = Chroma(
            persist_directory="./chroma_db",  # ë””ìŠ¤í¬ ê¸°ë°˜ ì €ì¥ì†Œ ì‚¬ìš©
            embedding_function=embeddings_model,
            collection_metadata={"hnsw:space": "cosine"},
        )
        logger.info("âœ… ChromaDB loaded successfully.")

        retriever = vectorstore.as_retriever()

        # Define prompt
        prompt = PromptTemplate.from_template(
        """
        You are an assistant for question-answering tasks.
        Use the following pieces of retrieved context to answer the question.
        If you don't know the answer, just say that you don't know.
        Please answer the question in Korean.
    
        *ì•„ë˜ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:*
        *ë°˜ë“œì‹œ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.*
        *ì§ˆë¬¸ê³¼ ë¬´ê´€í•œ ë‹µë³€ì„ í•˜ì§€ ë§ˆì„¸ìš”.*
        *ì¶”ì¸¡í•˜ì§€ ë§ê³  ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.*
        *ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ 'ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ'ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.*
        *ë‹µë³€ì˜ ë§ˆì§€ë§‰ì— ë¬¸ì„œì˜ ì œëª©ì„ ì²¨ë¶€í•˜ì„¸ìš”*
        *ë¬¸ì„œ ì œëª©ê³¼ ë‚´ìš© ì‚¬ì´ì— <br> íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¤„ ë°”ê¿ˆì„ ì¶”ê°€í•˜ì„¸ìš”.*

        #Question:
        {question}

        #Context:
        {context}

        #title:
        {title}

        #Answer:
        """
        )
       
        llm = ChatOpenAI(
            openai_api_key= "" #Your OpenAI API Key,
            temperature=0.1,
            model_name="gpt-3.5-turbo",
            request_timeout=10
        )

        logger.info("âœ… LangChain components initialized.")

        chain = (
            {"context": retriever, "question": RunnablePassthrough(), "title": retriever}
            | prompt
            | llm
        )

        logger.info("ğŸš€ LangChain RAG chain created successfully.")

        # Assign custom retriever to the retriever's _retrieve method
        retriever._retrieve = custom_retriever

        return chain

    except Exception as e:
        logger.error(f"âŒ Error creating LangChain chain with RAG: {e}")
        raise RuntimeError("Failed to create LangChain chain with RAG")
