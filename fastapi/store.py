# ì—…ë¡œë“œëœ PDF íŒŒì¼ì„ ì„ë² ë”©í•˜ì—¬ ChromaDBì— ì €ì¥
import os
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from chromadb import PersistentClient
import fitz
from langchain.schema import Document
from chromadb.config import Settings
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# FastAPI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
app = FastAPI()

# CORSMiddlewareëŠ” Cross-Origin Resource Sharing (CORS) ë¥¼ ì„¤ì •í•˜ëŠ” ë¯¸ë“¤ì›¨ì–´ë¡œ, 
# ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ë‹¤ë¥¸ ë„ë©”ì¸ ê°„ ìš”ì²­ì„ í—ˆìš©í• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ë„ë©”ì¸ í—ˆìš© (í•„ìš” ì‹œ íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©)
    allow_credentials=True, # ì¸ì¦ ì •ë³´ë¥¼ í¬í•¨í•œ ìš”ì²­(ì˜ˆ: ì¿ í‚¤, ì¸ì¦ í—¤ë” ë“±)ì„ í—ˆìš©
    allow_methods=["*"], # ëª¨ë“  HTTP ë©”ì„œë“œ(GET, POST, PUT, DELETE ë“±) í—ˆìš©
    allow_headers=["*"], # ëª¨ë“  ìš”ì²­ í—¤ë”ë¥¼ í—ˆìš©
)

# ìš”ì²­ ë°ì´í„° ëª¨ë¸
class StoreRequest(BaseModel):
    pdf_directory: str
    
@app.on_event("startup")
async def startup_event():
    # ì„ë² ë”© ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ app.stateì— ì €ì¥
    app.state.embeddings_model = get_embeddings_model()
    print("ì„ë² ë”© ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

# ------- ë¬¸ì„œ ë“±ë¡ë˜ë©´ ë²¡í„° ì €ì¥ -------
@app.post('/store')
async def docs_store(request: StoreRequest):
    pdf_directory = request.pdf_directory
    print("ìš”ì²­ ê²½ë¡œ:", pdf_directory)

    chroma_db_path = "./chroma_db"  # ChromaDB ì €ì¥ ê²½ë¡œ

    # PDF ë¡œë“œ ë° ë¶„í• 
    split_documents = load_and_split_pdf(pdf_directory)  

    # app.stateì—ì„œ ì„ë² ë”© ëª¨ë¸ ì¬ì‚¬ìš©
    embedding_model = app.state.embeddings_model

    # ì„ë² ë”© & ChromaDB ì €ì¥
    vectorstore = embed_and_store_in_chroma(split_documents, embedding_model, chroma_db_path)
    
    return {"message": "ì—…ë¡œë“œ ë° ë²¡í„° ë³€í™˜ ì™„ë£Œ!"}

# ì„ë² ë”©í•  ëª¨ë¸ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
def get_embeddings_model(model_name='jhgan/ko-sroberta-nli', device='cpu'):
    """
    Hugging Faceì˜ ì‚¬ì „í•™ìŠµëœ ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜.
    :param model_name: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ì´ë¦„
    :param device: ì‹¤í–‰í•  ë””ë°”ì´ìŠ¤ ('cpu' ë˜ëŠ” 'cuda')
    :return: HuggingFaceEmbeddings ê°ì²´
    """
    embeddings_model = HuggingFaceEmbeddings(
        # í•œêµ­ì–´ ìì—°ì–´ ì¶”ë¡  ìµœì í™”ëœ  ko-sroberta ëª¨ë¸
        model_name=model_name,
        # CPUì—ì„œ ì‹¤í–‰ë˜ë„ë¡ ì„¤ì •
        model_kwargs={'device': device},
        # ì„ë² ë”©ì„ ì •ê·œí™”, ë²¡í„°ê°€ ê°™ì€ ë²”ìœ„ì˜ ê°’ì„ ê°–ë„ë¡ í•¨. (ìœ ì‚¬ë„ ê³„ì‚°ì‹œ ì¼ê´€ì„± ë†’ì„)
        encode_kwargs={'normalize_embeddings': True},
    )
    return embeddings_model

# ì—…ë¡œë“œ íŒŒì¼ì„ ë¶„í• í•˜ì—¬ all_split_documents ë¦¬ìŠ¤íŠ¸ì— ë°˜í™˜
def load_and_split_pdf(pdf_path: str, chunk_size=500, chunk_overlap=200):
    """
    ì§€ì •ëœ PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³ , ê° í˜ì´ì§€ë³„ë¡œ ë¶„í• í•˜ì—¬ í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
    :param pdf_path: PDF íŒŒì¼ ê²½ë¡œ
    :param chunk_size: í…ìŠ¤íŠ¸ ë¶„í•  í¬ê¸°
    :param chunk_overlap: í…ìŠ¤íŠ¸ ë¶„í•  ì‹œ ì˜¤ë²„ë© í¬ê¸°
    :return: ë¶„í• ëœ í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
    """
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,  # ë¬¸ìì—´ ê¸¸ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
    )
    """  
    ê¸°ë³¸ì ìœ¼ë¡œ ë¬¸ì¥ì„ ëŠì–´ì„œ ë¶„í• í•˜ë ¤ê³  ì‹œë„í•©ë‹ˆë‹¤.
    í•˜ì§€ë§Œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸° ì–´ë µë‹¤ë©´, ë‹¨ë½(ì¤„ë°”ê¿ˆ \n\n)ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    ê·¸ë˜ë„ ì ì ˆí•˜ê²Œ ë‚˜ëˆ„ê¸° ì–´ë µë‹¤ë©´, ê³µë°±( )ì„ ê¸°ì¤€ìœ¼ë¡œ ì¬ê·€ì ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    ìµœí›„ì˜ ë°©ë²•ìœ¼ë¡œ, ì§€ì •í•œ chunk_size í¬ê¸°ë§Œí¼ ê°•ì œë¡œ ë¶„í• í•©ë‹ˆë‹¤. 
    """
    
    all_split_documents = []  # ëª¨ë“  ë¶„í• ëœ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

    print(f"Processing: {pdf_path}")

    # 1. PDF ë¡œë“œ
    doc = fitz.open(pdf_path)  # PyMuPDFë¡œ PDF ì—´ê¸°
    print(f"  - Loaded {len(doc)} pages.")

    # 2. ê° í˜ì´ì§€ë³„ë¡œ í…ìŠ¤íŠ¸ ë¶„í• 
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        page_text = page.get_text()  # í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        # í…ìŠ¤íŠ¸ë¥¼ ë¶„í• 
        split_texts = text_splitter.split_text(page_text)
        
        # Document ê°ì²´ ìƒì„± (ë©”íƒ€ë°ì´í„° í¬í•¨)
        for chunk in split_texts:
            all_split_documents.append(Document(
                page_content=chunk,
                metadata={"Title": pdf_path.split("/")[-1], "Page": page_num + 1}
            ))

    print(f"Total {len(all_split_documents)} text chunks from the PDF.")
    
    return all_split_documents

# ë¶„í• ëœ docsë¥¼ ì„ë² ë”©í•˜ì—¬ ChromaDBì— ì €ì¥
def embed_and_store_in_chroma(split_documents, embeddings_model, chroma_db_path="./chroma_db"):
    """
    ë¶„í• ëœ ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ê³  ChromaDBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜.
    :param split_documents: Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ (ë©”íƒ€ë°ì´í„° í¬í•¨)
    :param embeddings_model: HuggingFace ì„ë² ë”© ëª¨ë¸ ê°ì²´
    :param chroma_db_path: ChromaDBê°€ ì €ì¥ë  ë””ë ‰í† ë¦¬ ê²½ë¡œ
    :return: ìƒì„±ëœ Chroma ë²¡í„°ìŠ¤í† ì–´ ê°ì²´
    """
    if not split_documents:
        print("No documents to process.")
        return None
    
    # âœ… PersistentClientë¥¼ ì‚¬ìš©í•˜ì—¬ ChromaDB ì €ì¥
    chroma_client = PersistentClient(path=chroma_db_path)
    vectorstore = Chroma(
        client=chroma_client,
        embedding_function=embeddings_model,
        collection_metadata={"hnsw:space": "cosine"},
    )

    #âœ… PersistentClientë¥¼ ì‚¬ìš©í•˜ì—¬ ë””ìŠ¤í¬ ê¸°ë°˜ìœ¼ë¡œ ChromaDBë¥¼ ê´€ë¦¬
    #âœ… embedding_function=embeddings_model: ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
    #âœ… collection_metadata={"hnsw:space": "cosine"}: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ ì„¤ì •

    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
    existing_docs = vectorstore.get(include=["metadatas"])
    doc_ids_to_delete = [
        doc_id for doc_id, metadata in zip(existing_docs["ids"], existing_docs["metadatas"])
        if metadata and "Title" in metadata and metadata["Title"] == split_documents[0].metadata["Title"]
    ]
    # ê¸°ì¡´ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ê°€ ì¡´ì¬í•˜ê³ (metadata), í•´ë‹¹ ë¬¸ì„œì— Title í‚¤ê°€ ìˆìœ¼ë©°, ìƒˆë¡œ ì¶”ê°€í•˜ë ¤ëŠ” ë¬¸ì„œì˜ Titleê³¼ ë™ì¼í•œ ê²½ìš°ì— True
    
    
    if doc_ids_to_delete:
        print(f"ğŸ—‘ Deleting {len(doc_ids_to_delete)} existing vectors for {split_documents[0].metadata['Title']}")
        vectorstore.delete(doc_ids_to_delete)

    # ìƒˆë¡œìš´ ë¬¸ì„œ ì €ì¥
    vectorstore.add_documents(split_documents)

    print(f"âœ… Successfully stored {len(split_documents)} document embeddings in ChromaDB at {chroma_db_path}.")
    return vectorstore
