## AI 안전관리자  프로젝트

    
   * Q1. 실시간 영상을 받아오기 위해서 접속시 소켓이 연결되고, 종료시 소켓이 해제되는 식으로 구상해야 할까요?

   * A1  실시간 영상을 받아오고 객체 탐지를 수행하며 결과를 보여주는 시스템을 설계하려면 다음과 같은 구성


      1. 소켓 연결 및 해제
      
         * 실시간 데이터 처리를 위해 소켓 기반의 통신은 일반적입니다. 클라이언트-서버 구조에서

           - 접속 시: 클라이언트가 서버에 연결하여 실시간으로 영상을 전송받습니다.
           - 종료 시: 연결을 닫아 리소스를 해제합니다.

      2. 파이프라인 설계

         * 다음과 같은 파이프라인이 일반적

             1. 실시간 영상 수신

                - 클라이언트에서 웹캠, CCTV, 또는 다른 영상 스트림을 캡처.
                - 소켓을 통해 서버로 전송 (e.g., WebSocket, TCP/UDP).
                - OpenCV 또는 FFmpeg를 사용하여 프레임 단위로 영상을 처리.

             2. 객체 탐지 수행

               - 수신된 프레임을 바로 딥러닝 모델 (e.g., YOLO, SSD, Faster R-CNN 그외)에 전달.
               - YOLOv8(Ultralytics), OpenCV DNN 모듈, 또는 TensorFlow와 PyTorch 기반 모델 사용.
               - 탐지된 객체의 좌표와 클래스 라벨을 반환.

            3. 결과 시각화 및 출력

            - 탐지 결과를 실시간으로 오버레이하여 영상에 표시.
            - 탐지 현황 (예: 객체의 이름, 좌표, 개수)을 UI 또는 콘솔로 출력.

           4. 종료 시 리소스 해제

            - 소켓 연결 해제, OpenCV 리소스 및 GPU 메모리 정리.


  3. 구체적인 작동 방식

     * 아래는 파이썬으로 간단한 워크플로를 설명한 코드

       3.1 서버 코드 (소켓 + 객체 탐지)

```python
import cv2
import socket
import pickle
import struct
from ultralytics import YOLO  # YOLOv8 모델 사용

# 서버 설정
server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
server_socket.bind(('0.0.0.0', 8080))  # 모든 IP에서 접속 허용
server_socket.listen(5)
print("서버 대기 중...")

# YOLO 모델 로드
model = YOLO("yolov8n.pt")  # 사전 학습된 YOLOv8 모델

while True:
    client_socket, addr = server_socket.accept()
    print(f"클라이언트 {addr} 연결됨")

    data = b""
    payload_size = struct.calcsize("Q")

    while True:
        try:
            # 클라이언트로부터 영상 프레임 데이터 수신
            while len(data) < payload_size:
                packet = client_socket.recv(4096)
                if not packet: break
                data += packet
            
            packed_msg_size = data[:payload_size]
            data = data[payload_size:]
            msg_size = struct.unpack("Q", packed_msg_size)[0]

            while len(data) < msg_size:
                data += client_socket.recv(4096)

            frame_data = data[:msg_size]
            data = data[msg_size:]

            # 수신한 프레임 디코딩
            frame = pickle.loads(frame_data)
            frame = cv2.imdecode(frame, cv2.IMREAD_COLOR)

            # 객체 탐지 수행
            results = model(frame)

            # 탐지된 객체 결과 표시
            for result in results.xyxy[0]:  # x1, y1, x2, y2, confidence, class
                x1, y1, x2, y2, conf, cls = result
                label = model.names[int(cls)]
                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
                cv2.putText(frame, f"{label} {conf:.2f}", (int(x1), int(y1) - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            # 결과 영상 클라이언트에 전송
            cv2.imshow("객체 탐지", frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

        except Exception as e:
            print(f"오류 발생: {e}")
            break

    client_socket.close()
    print(f"클라이언트 {addr} 연결 종료")

```


     3-2 클라이언트 코드 (영상 전송)

```python
import cv2
import socket
import pickle
import struct

# 클라이언트 설정
client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
client_socket.connect(('127.0.0.1', 8080))  # 서버 IP와 포트

cap = cv2.VideoCapture(0)  # 웹캠 열기

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # 프레임 인코딩
    encoded_frame = cv2.imencode('.jpg', frame)[1]
    data = pickle.dumps(encoded_frame, protocol=pickle.HIGHEST_PROTOCOL)
    message = struct.pack("Q", len(data)) + data

    # 서버로 전송
    client_socket.sendall(message)

cap.release()
client_socket.close()

```


4. 결과 및 확장

   * 탐지 결과 시각화    : 영상 스트림에 실시간으로 탐지 결과를 오버레이.
   * 현황 출력           : 탐지된 객체 정보를 서버 로그나 별도의 웹 대시보드에 출력 가능.
   * 모델 선택           : 성능에 따라 YOLOv8 외에도 TensorFlow Lite나 Edge AI 모델로 대체 가능.
   * 다중 클라이언트 지원: select나 asyncio를 사용하여 다중 연결 관리





## RTSP + FFmpeg

 -  RTSP를 사용하여 실시간 스트리밍 영상을 가져오고,이를 FFmpeg와 함께 파이썬 클라이언트에서 처리하려면 다음 단계를 따를 수 있습니다.
    RTSP는 일반적으로 CCTV, IP 카메라 또는 실시간 스트리밍 서비스에서 사용됩니다.


1. RTSP 스트리밍과 FFmpeg의 역할

  * RTSP 스트림 : IP 카메라나 스트리밍 서버에서 실시간 영상을 가져옵니다.
  * FFmpeg      : RTSP 스트림을 디코딩하고, 필요한 경우 처리하거나 변환합니다.



2. 필요한 라이브러리

```bash
pip install opencv-python-headless ffmpeg-python

```

3. RTSP 스트림 클라이언트 코드
   다음은 RTSP 스트림을 가져와 소켓을 통해 서버로 전송하는 코드


```python
import cv2
import socket
import pickle
import struct
import subprocess

# 클라이언트 설정
client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
client_socket.connect(('127.0.0.1', 8080))  # 서버 IP와 포트

# RTSP URL 입력 (카메라 또는 서버에서 제공)
RTSP_URL = "rtsp://username:password@<camera_ip>:554/stream1"

# FFmpeg 명령어로 RTSP 스트림 처리
ffmpeg_command = [
    'ffmpeg',
    '-i', RTSP_URL,              # RTSP 스트림 입력
    '-f', 'rawvideo',            # 원시 비디오 출력 형식
    '-pix_fmt', 'bgr24',         # OpenCV와 호환되는 픽셀 포맷
    '-an',                       # 오디오 제외
    '-'
]

# FFmpeg subprocess 실행
process = subprocess.Popen(ffmpeg_command, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, bufsize=10**8)

while True:
    try:
        # FFmpeg에서 프레임 읽기
        raw_frame = process.stdout.read(640 * 480 * 3)  # 640x480 해상도, 3채널 (BGR)
        if not raw_frame:
            break

        # OpenCV로 프레임 디코딩
        frame = cv2.imdecode(
            cv2.UMat.frombuffer(raw_frame, dtype='uint8').reshape((480, 640, 3)),
            cv2.IMREAD_COLOR
        )

        # 소켓 전송 준비
        encoded_frame = cv2.imencode('.jpg', frame)[1]
        data = pickle.dumps(encoded_frame, protocol=pickle.HIGHEST_PROTOCOL)
        message = struct.pack("Q", len(data)) + data

        # 서버로 전송
        client_socket.sendall(message)

        # 클라이언트에서 로컬 미리보기 (선택 사항)
        cv2.imshow("RTSP Stream", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    except Exception as e:
        print(f"오류 발생: {e}")
        break

# 리소스 정리
process.terminate()
client_socket.close()
cv2.destroyAllWindows()

```


4. 코드 설명

    1.  RTSP 스트림 URL

       * RTSP_URL은 카메라 또는 서버에서 제공하는 RTSP 주소를 입력해야 합니다.

           - 예

             ```bash
                rtsp://admin:password@192.168.0.101:554/stream1
             ```

    2. FFmpeg 명령어

            - FFmpeg는 RTSP 스트림을 가져오고, rawvideo 포맷으로 디코딩하여 OpenCV가 읽을 수 있는 형식으로 출력.

    3. FFmpeg와 OpenCV 통합

           - FFmpeg의 출력을 OpenCV로 받아와 디코딩하고, 이를 서버로 전송.

    4. 서버로 전송

          - pickle로 직렬화하여 소켓을 통해 프레임을 전송합니다.
          - 전송 중에 소켓 연결이 끊기거나 에러가 발생하면 예외 처리로 종료.

    5. 로컬 미리보기

          - cv2.imshow를 사용하여 클라이언트에서 실시간으로 스트림을 확인.


5. FFmpeg 없이 OpenCV로 RTSP 스트림 처리 (대안)

        - OpenCV 자체에서도 RTSP를 지원

```python
cap = cv2.VideoCapture("rtsp://username:password@<camera_ip>:554/stream1")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # 프레임 전송
    encoded_frame = cv2.imencode('.jpg', frame)[1]
    data = pickle.dumps(encoded_frame, protocol=pickle.HIGHEST_PROTOCOL)
    message = struct.pack("Q", len(data)) + data
    client_socket.sendall(message)

    # 로컬 미리보기
    cv2.imshow("RTSP Stream", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
client_socket.close()
cv2.destroyAllWindows()

```



























